# Concurrency Patterns in Go #

## Confinement ##

For safe operations, we have different options:

- synchronization primitives for sharing memory (e.g. `sync.Mutex`)
- synchronization via communicating (e.g. `channels`)

and

- immutable data
- data protected by confinement

---

Confinement can be of two types:
- ad hoc
- lexical

Adhoc confinement is hard to control.Maybe, it can be used if your team is actively using static analysis. In the example below, `data` is used only by the `loopData` function. 

```
data := make([]int, 4)

loopData := func(handleData chan<- int) {
    defer close(handleData)
    for i := range data {
        handleData <- data[i]
    }
}

handleData := make(chan int)
go loopData(handleData)

for num := range handleData {
    fmt.Println(num)
}
```

Lexical confinement involves using lexical scope to expose the correct data.

```
chanOwner := func() <-chan int {
    results := make(chan int, 5)
    go func() {
        defer close(results)
        for i := 0; i <= 5; i++ {
            results <- i
        }
    }()
    return results
}

consumer := func(results <-chan int) {
    for result := range results {
        fmt.Printf("Received: %d\n", result)
    }
    fmt.Println("Done receiving!")
}

results := chanOwner()
consumer(results)
```

## The for-select Loop ##

This is regular pattern in Go. It is usually used to:

- Sending iteration variables out on an channel
```
for _, s := range []string{"a", "b", "c"} {
    select {
    case <-done:
        return
    case stringStream <- s:
    }
}
```
- Looping infinitely waiting to be stopped
```
// Embedding the work in a `default` clause.
for {
    select {
    case <-done:
        return
    default:
    // Do non-preemptable work
    }
}

// Keeping the `select` statement as short as possible.
for {
    select {
    case <-done:
        return
    default:
    }
// Do non-preemptable work
}
```

## Preventing Goroutine Leaks ##

We don't want to have a goroutine that remains throughout the lifetime of the process by mistake. Like here:
```
doWork := func(strings <-chan string) <-chan interface{} {
    completed := make(chan interface{})
    go func() {
        defer fmt.Println("doWork exited.")
        defer close(completed)
        for s := range strings {
            // Do something interesting
            fmt.Println(s)
        }
    }()
    return completed
}

doWork(nil)
// Perhaps more work is done here
fmt.Println("Done.")
```

A way to mitigate this is to use a signal for the cancellation of the child goroutines through its parent. For example:

```
doWork := func(
    done <-chan interface{},
    strings <-chan string,
    ) <-chan interface{} {
        terminated := make(chan interface{})
        go func() {
            defer fmt.Println("doWork exited.")
            defer close(terminated)
            for {
                select {
                case s := <-strings:
                    fmt.Println(s)
                case <-done:
                    return
                }
            }
        }()
        return terminated
}

done := make(chan interface{})
terminated := doWork(done, nil)

go func() {
    // Cancel the operation after 1 second.
    time.Sleep(1 * time.Second)
    fmt.Println("Canceling doWork goroutine...")
    close(done)
}()

<-terminated
fmt.Println("Done.")
```
---

In the same vein, a goroutine can be blocked when attempting to write a value to a channel. Like here:

```
newRandStream := func() <-chan int {
    randStream := make(chan int)
    go func() {
        defer fmt.Println("newRandStream closure exited.") 
        defer close(randStream)
        for {
            randStream <- rand.Int()
        }
    }()

    return randStream
}

randStream := newRandStream()
fmt.Println("3 random ints:")
for i := 1; i <= 3; i++ {
    fmt.Printf("%d: %d\n", i, <-randStream)
}
```

The `fmt.Println` statement never gets run. A possible solution would be to provide the producer goroutine with a channel informing it to exit:

```
newRandStream := func(done <-chan interface{}) <-chan int { randStream := make(chan int)
    go func() {
        defer fmt.Println("newRandStream closure exited.")
        defer close(randStream)
        for {
            select {
            case randStream <- rand.Int():
            case <-done:
                return
            }
        }
    }()

    return randStream
}

done := make(chan interface{})
randStream := newRandStream(done)
fmt.Println("3 random ints:")
for i := 1; i <= 3; i++ {
    fmt.Printf("%d: %d\n", i, <-randStream)
}
close(done)

// Simulate ongoing work
time.Sleep(1 * time.Second)
```

## The or-channel ##

Sometimes you want to combine one or more `done` channels into a single `done ` channel that closes if any of its component channels close. You can write a `select` statement that does this..if you know the number of `done` channels you're working with by runtime. 

```
var or func(channels ...<-chan interface{}) <-chan interface{}
or = func(channels ...<-chan interface{}) <-chan interface{} { 
    switch len(channels) {
    case 0: 
        return nil
    case 1: 
        return channels[0]
    }

    orDone := make(chan interface{})
    go func() { 
        defer close(orDone)

        switch len(channels) {
        case 2: 
            select {
            case <-channels[0]:
            case <-channels[1]:
            }
        default: 
            select {
            case <-channels[0]:
            case <-channels[1]:
            case <-channels[2]:
            case <-or(append(channels[3:], orDone)...): 
            }
        }
    }()
    return orDone
}
sig := func(after time.Duration) <-chan interface{} { 
    c := make(chan interface{})
    go func() {
        defer close(c)
        time.Sleep(after)
    }()
    return c
}

start := time.Now() 
<-or(
    sig(2*time.Hour),
    sig(5*time.Minute),
    sig(1*time.Second),
    sig(1*time.Hour),
    sig(1*time.Minute),
)
fmt.Printf("done after %v", time.Since(start)) 
```

This can be helpful especially at the intersection of modules in your system - where you have multiple conditions for cancelling trees of goroutines through the call stack.

## Error Handling ##

"Who should be responsible for handling the error?"

This rings true especially in concurrent processes with parent-sibling relationships. The complexity rises and we need to handle things in a logical manner. Like here:

```
type Result struct { 
    Error    error
    Response *http.Response
}

checkStatus := func(done <-chan interface{}, urls ...string) <-chan Result { 
    results := make(chan Result)
    go func() {
        defer close(results)

        for _, url := range urls {
            var result Result
            resp, err := http.Get(url)
            result = Result{Error: err, Response: resp} 
            select {
            case <-done:
                return
            case results <- result: 
            }
        }
    }()
    return results
}

done := make(chan interface{})
defer close(done)

urls := []string{"https://www.google.com", "https://badhost"}
for result := range checkStatus(done, urls...) {
    // Another approach would be to introduce an errCount variable to keep track of 
    // how many errors you get and stop the loop after a specific number.
    if result.Error != nil { 
        fmt.Printf("error: %v", result.Error)
        continue
    }
    fmt.Printf("Response: %v\n", result.Response.Status)
}
```

Errors should be considered first-class citizens when constructing values to return from goroutines. Especially in more complex scenarios. Good ideas to have tight coupling with the results and use the same lines of communications..

## Pipelines ##

Pipeline is another tool to form abstractions in your system. It's a series of things that take the data, perform an operation on it and pass the data back out - these being called *pipeline stages*. 

Pipeline stages can process data through:

- Batch processing - operating on chunks of data at once.
- Stream processing - operating on one discrete value at a time.

```
// Batch processing.
multiply := func(values []int, multiplier int) []int {
    multipliedValues := make([]int, len(values))
    for i, v := range values {
        multipliedValues[i] = v * multiplier
    }
    return multipliedValues
}
add := func(values []int, additive int) []int {
    addedValues := make([]int, len(values))
    for i, v := range values {
        addedValues[i] = v + additive
    }
    return addedValues
}
ints := []int{1, 2, 3, 4}
for _, v := range multiply(add(multiply(ints, 2), 1), 2) {
    fmt.Println(v)
}


// Stream processing.
multiply := func(value, multiplier int) int {
    return value * multiplier
}

add := func(value, additive int) int {
    return value + additive
}

ints := []int{1, 2, 3, 4}
for _, v := range ints {
    fmt.Println(multiply(add(multiply(v, 2), 1), 2))
}

```
---

### Best practices for constructing pipelines ###

The example below encompasses most of the things we talked about in the previous chapters. For example to use the `done` channel in order to gracefully signal multiple channels to exit when we want to and prevent goroutine leaks.

```
generator := func(done <-chan interface{}, integers ...int) <-chan int {
    intStream := make(chan int)
    go func() {
        defer close(intStream)
        for _, i := range integers {
            select {
            case <-done:
                return
            case intStream <- i:
            }
        }
    }()
    return intStream
}

multiply := func(
    done <-chan interface{},
    intStream <-chan int,
    multiplier int,
) <-chan int {
    multipliedStream := make(chan int)
    go func() {
        defer close(multipliedStream)
        for i := range intStream {
            select {
            case <-done:
                return
            case multipliedStream <- i * multiplier:
            }
        }
    }()
    return multipliedStream
}

add := func(
    done <-chan interface{},
    intStream <-chan int,
    additive int,
) <-chan int {
    addedStream := make(chan int)
    go func() {
        defer close(addedStream)
        for i := range intStream {
            select {
            case <-done:
                return
            case addedStream <- i + additive:
            }
        }
    }()
    return addedStream
}

done := make(chan interface{})
defer close(done)

intStream := generator(done, 1, 2, 3, 4)
pipeline := multiply(done, add(done, multiply(done, intStream, 2), 1), 2)

for v := range pipeline {
    fmt.Println(v)
}
```

Here, the `generator` function converts a discrete set of values into a stream of data on a channel. After all, that's why it's called a `generator`. Another key thing to note is that each stage of the pipeline is executing concurrently.

## Fan-Out, Fan-In ##

Computational intensive/ blocked stages in the pipeline can significantly increase the time it takes the program to complete. You can parallelize pulls from an upstream stage by using *fan-out, fan-in*. 

Fan-out - starting multiple goroutines to handle the input from a pipeline

Fan-in - combining said results into one channel. You essentially multiplex these stages by fanning-in.

---

This pattern is especially useful if:
- it doesn't rely on values that the stage has calculated before (i.e. order-independence)
- it takes a long time to run

We can use the previous example from pipelines and update it with some additional logic, construnctive a naive prime finder.

```
repeatFn := func(
    done <-chan interface{},
    fn func() interface{},
) <-chan interface{} {
    valueStream := make(chan interface{})
    go func() {
        defer close(valueStream)
        for {
            select {
            case <-done:
                return
            case valueStream <- fn():
            }
        }
    }()
    return valueStream
}
take := func(
    done <-chan interface{},
    valueStream <-chan interface{},
    num int,
) <-chan interface{} {
    takeStream := make(chan interface{})
    go func() {
        defer close(takeStream)
        for i := 0; i < num; i++ {
            select {
            case <-done:
                return
            case takeStream <- <-valueStream:
            }
        }
    }()
    return takeStream
}
toInt := func(done <-chan interface{}, valueStream <-chan interface{}) <-chan int {
    intStream := make(chan int)
    go func() {
        defer close(intStream)
        for v := range valueStream {
            select {
            case <-done:
                return
            case intStream <- v.(int):
            }
        }
    }()
    return intStream
}
primeFinder := func(done <-chan interface{}, intStream <-chan int) <-chan interface{} {
    primeStream := make(chan interface{})
    go func() {
        defer close(primeStream)
        for integer := range intStream {
            integer -= 1
            prime := true
            for divisor := integer - 1; divisor > 1; divisor-- {
                if integer%divisor == 0 {
                    prime = false
                    break
                }
            }

            if prime {
                select {
                case <-done:
                    return
                case primeStream <- integer:
                }
            }
        }
    }()
    return primeStream
}
fanIn := func(
    done <-chan interface{},
    channels ...<-chan interface{},
) <-chan interface{} { 
    var wg sync.WaitGroup 
    multiplexedStream := make(chan interface{})

    multiplex := func(c <-chan interface{}) { 
        defer wg.Done()
        for i := range c {
            select {
            case <-done:
                return
            case multiplexedStream <- i:
            }
        }
    }

    // Select from all the channels
    wg.Add(len(channels)) 
    for _, c := range channels {
        go multiplex(c)
    }

    // Wait for all the reads to complete
    go func() { 
        wg.Wait()
        close(multiplexedStream)
    }()

    return multiplexedStream
}

done := make(chan interface{})
defer close(done)

start := time.Now()

rand := func() interface{} { return rand.Intn(50000000) }

randIntStream := toInt(done, repeatFn(done, rand))

numFinders := runtime.NumCPU()
fmt.Printf("Spinning up %d prime finders.\n", numFinders)
finders := make([]<-chan interface{}, numFinders)
fmt.Println("Primes:")
for i := 0; i < numFinders; i++ {
    finders[i] = primeFinder(done, randIntStream)
}

for prime := range take(done, fanIn(done, finders...), 10) {
    fmt.Printf("\t%d\n", prime)
}

fmt.Printf("Search took: %v", time.Since(start))
```

## The or-done-channel ## 

This pattern is useful when you can't make assertions about how a channel will behave. Especially if you work with channels from disparate parts of your system - you don't always know if the fact that your goroutine was canceles means the channel you're reading from will be cancelled as well.

We can use `select` statements that also select from a `done` channel. 

```
// Instead of having this:
for val := range myChan {
    // Do something with val
}


// That will probably explode into this:
loop:
for {
    select {
    case <-done:
        break loop
    case maybeVal, ok := <-myChan:
        if ok == false {
            return // or maybe break from for
        }
        // Do something with val
    }
}
```

You can have:

```
orDone := func(done, c <-chan interface{}) <-chan interface{} {
    valStream := make(chan interface{})
    go func() {
        defer close(valStream)
        for {
            select {
            case <-done:
                return
            case v, ok := <-c:
                if ok == false {
                    return
                }
                select {
                case valStream <- v:
                case <-done:
                }
            }
        }
    }()
    return valStream
}

// So we can still have somthing like this:
for val := range orDone(done, myChan) {
    // Do something with val
}
```

## The tee-channel ## 

This is useful if you want to split values coming in from a channel and send them off into two separate areas in your codebase.

```
tee := func(
    done <-chan interface{},
    in <-chan interface{},
    ) (_, _ <-chan interface{}) { <-chan interface{}) {
    out1 := make(chan interface{})
    out2 := make(chan interface{})
    go func() {
        defer close(out2)
        for val := range orDone(done, in) {
            var out1, out2 = out1, out2
            for i := 0; i < 2; i++ {
                select {
                case <-done:
                case out1<-val:
                    out1 = nil
                case out2<-val:
                    out2 = nil
                }
            }
        }
    }()
    return out1, out2
}

done := make(chan interface{})
defer close(done)

// Notice how out1 and out2 are tightly coupled.
out1, out2 := tee(done, take(done, repeat(done, 1, 2), 4))

for val1 := range out1 {
    fmt.Printf("out1: %v, out2: %v\n", val1, <-out2)
}
```

## The bridge-channel ## 

This comes in handy when you want to consume values from a sequence of channels. It can be done through a function that can destructure the channel of channels into a simple channel - a technique called *bridging* channels.

```
orDone := func(done, c <-chan interface{}) <-chan interface{} {
    valStream := make(chan interface{})
    go func() {
        defer close(valStream)
        for {
            select {
            case <-done:
                return
            case v, ok := <-c:
                if ok == false {
                    return
                }
                select {
                case valStream <- v:
                case <-done:
                }
            }
        }
    }()
    return valStream
}
bridge := func(
    done <-chan interface{},
    chanStream <-chan <-chan interface{},
) <-chan interface{} {
    valStream := make(chan interface{}) 
    go func() {
        defer close(valStream)
        for { 
            var stream <-chan interface{}
            select {
            case maybeStream, ok := <-chanStream:
                if ok == false {
                    return
                }
                stream = maybeStream
            case <-done:
                return
            }
            // This provides us with an unbroken stream of values.
            for val := range orDone(done, stream) { 
                select {
                case valStream <- val:
                case <-done:
                }
            }
        }
    }()
    return valStream
}
genVals := func() <-chan <-chan interface{} {
    chanStream := make(chan (<-chan interface{}))
    go func() {
        defer close(chanStream)
        for i := 0; i < 10; i++ {
            stream := make(chan interface{}, 1)
            stream <- i
            close(stream)
            chanStream <- stream
        }
    }()
    return chanStream
}

for v := range bridge(nil, genVals()) {
    fmt.Printf("%v ", v)
}
```

## Queuing ## 

If you want to accept work even if the pipeline is not ready for more you can use `queuing`.

It is important to note that adding queuing prematurely can hide synchronization issues. An interesting thing that is touched upon is the notion that queuing will almost never speed up the total runtime of the program; it will only allow the program to behave differently.

Situations in which you can increase performance by using queuing:
- if batching requests in a stage saves time
- if delays in a stage produce a feedback loop into the system

In the first situation, buffers input in something faster (e.g. memory) than it is designed to send to (e.g. disk). Like here:

```
func BenchmarkUnbufferedWrite(b *testing.B) {
    performWrite(b, tmpFileOrFatal())
}

func BenchmarkBufferedWrite(b *testing.B) {
    bufferredFile := bufio.NewWriter(tmpFileOrFatal())
    performWrite(b, bufio.NewWriter(bufferredFile))
}

func tmpFileOrFatal() *os.File {
    file, err := ioutil.TempFile("", "tmp")
    if err != nil {
        log.Fatal("error: %v", err)
    }
    return file
}

func performWrite(b *testing.B, writer io.Writer) {
    done := make(chan interface{})
    defer close(done)
    b.ResetTimer()
    for bt := range take(done, repeat(done, byte(0)), b.N) {
        writer.Write([]byte{bt.(byte)})
    }
}
```

Of course, when doing a quick benchmark test with `go test -bench=. src/concurrency-patterns-in-go/queuing/buffering_test.go`, the buffered write is faster. This is because in `bufio.Writer`, the writes are *queued* internally into a buffer until a sufficient chunk has been accumulated, and then the chunk is written out. The process is called...*chunking*.

---

In the second situation, the idea is often referred to as a *negative feedback loop*/ downward-spiral/ death-spiral. It usually happens where a delay in a stage causes more input into the pipeline which can lead to a systemic collapse of the system.

We can use Little's Law to predict the throughput of the system: `L=λW` . Where:
- L = the average number of units in the system.
- λ = the average arrival rate of units.
- W = the average time a unit spends in the system.

This applies to *stable* systems. These being where the *ingress* (rate that work enters the pipeline) is equal to *egress* (rate that exits the system). In the end, the pipeline will only be as fast as your slowest stage.

## The context Package ## 

This package satisfies the need to communicate extra information; why the cancellation is occuring or whether or not our function has a deadline by which it needs to complete.

The `Context` type will flow through your system much like a `done` channel does. It really servers two purposes:
- to provide an API for cancelling branches of your call-graph
- to provide a data-bag for transporting request-scoped data through your call-graph

`Context` is immutable (this protects the functions up the call stack from children cancelling the context). It manages cancellation by using functions such as:

```
func WithCancel(parent Context) (ctx Context, cancel CancelFunc)
func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc)
func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc)
```

Functions which take in a `Context` and return one as well. These close it's `Done` channel when the machine's clock advances past the given `deadline`, for example.

```
type Context interface {
    // Deadline returns the time when work done on behalf of this
    // context should be canceled. Deadline returns ok==false when no
    // deadline is set. Successive calls to Deadline return the same
    // results.
    Deadline() (deadline time.Time, ok bool)

    // Done returns a channel that's closed when work done on behalf
    // of this context should be canceled. Done may return nil if this
    // context can never be canceled. Successive calls to Done return
    // the same value.
    Done() <-chan struct{}

    // Err returns a non-nil error value after Done is closed. Err
    // returns Canceled if the context was canceled or
    // DeadlineExceeded if the context's deadline passed. No other
    // values for Err are defined. After Done is closed, successive
    // calls to Err return the same value.
    Err() error

    // Value returns the value associated with this context for key,
    // or nil if no value is associated with key. Successive calls to
    // Value with the same key returns the same result.
    Value(key interface{}) interface{}
}
```

To start the chain, the `context` package provides us with two functions to create empty instances of `Context`:

```
// Background simply returns an empty context.
func Background() Context

// TODO serves as a placeholder for when you don't know which
// Context to utilize, or if you expect your code to be provided
// with a Context, but the upstream code hasn't yet furnished one.
func TODO() Context
```

Regarding "transits processes and API boundaries": 
```
Use context values only for request-scoped data that transits processes and
API boundaries, not for passing optional parameters to functions.
```

We can use these heuristics:
1) The data should transit process or API boundaries.
If you generate the data in your process’ memory, it’s probably not a good candi‐
date to be request-scoped data unless you also pass it across an API boundary.

2) The data should be immutable.
If it’s not, then by definition what you’re storing did not come from the request.

3) The data should trend toward simple types.
If request-scoped data is meant to transit process and API boundaries, it’s much
easier for the other side to pull this data out if it doesn’t also have to import a
complex graph of packages.

4) The data should be data, not types with methods.
Operations are logic and belong on the things consuming this data.

5) The data should help decorate operations, not drive them.
If your algorithm behaves differently based on what is or isn’t included in its
Context, you have likely crossed over into the territory of optional parameters.