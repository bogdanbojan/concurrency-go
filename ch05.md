# Concurrency at Scale #

## Error Propagation ##

Go attempts to correct the ferrying up the stack of errors without much tought and ultimately dumping them in front of the user. It does this by forcing users to handle errors at every frame in the call stack..thus hindering the use of errors as second-class citizens to the system's control flow.

When errors occur, is useful to have these in mind:
- what happened
- when and where it occurred
- a friendly user-facing message
- how the user can get more information 

By default, no error will contain all of this information without your intervention. This leads to a framework that enables us to place all errors into one of two categories:
- bugs
- known edge cases(e.g. broken network connections, failed disk writes, etc)

Bugs being "raw" errors - which sometimes can be intentional especially in the first few iterations of the system.

A well-formed error could look like:

```
type MyError struct {
	Inner      error
	Message    string
	StackTrace string
	Misc       map[string]interface{}
}

func wrapError(err error, messagef string, msgArgs ...interface{}) MyError {
	return MyError{
		Inner:      err, 
		Message:    fmt.Sprintf(messagef, msgArgs...),
		StackTrace: string(debug.Stack()),        
		Misc:       make(map[string]interface{}),
	}
}

func (err MyError) Error() string {
	return err.Message
}
```

## Timeouts and Cancellation ##

Timeouts and cancelations are crucial in order to create a system with a behaviour you can understand. Cancellation being a natural response to a time out.

Why do we want time outs?
- System saturation (i.e. its ability to process requests is at capacity). When do we time out?: 
    - if we can't store the request
    - if the need for the request or the data is going stale
    - if the request is unlikely to be repeated when it is timed out

- Stale data
    - sometimes data has a window within which it must pe processed before relevant data is available or the need to process data has expired. We can use the `Context` package to manage that.

- Attempting to prevent deadlocks
    - It is not unreasonable, even recommended, to place timeouts on *all* of your concurrent operations to guarantee the system won't deadlock. There is a possibility that you introduce livelocks this way..

## Heartbeats ##

Heartbeats are a way for concurrent processes to signal life outside parties.

Two main different types:
- heartbeats that occur on a time interval
- hearbeats that occur at the beginning of a unit of work

They aren't that interesting normally, their utility becomes apparent when something is amiss with the goroutines and the regularity of the pulses changes. Also, you can prevent problems (like deadlocks) by using hearbeats and not relying on long timeouts.

A nice way to use this is when you write tests:
```
func DoWork(
	done <-chan interface{},
	nums ...int,
) (<-chan interface{}, <-chan int) {
	heartbeat := make(chan interface{}, 1)
	intStream := make(chan int)
	go func() {
		defer close(heartbeat)
		defer close(intStream)

		time.Sleep(2 * time.Second) 

		for _, n := range nums {
			select {
			case heartbeat <- struct{}{}:
			default:
			}

			select {
			case <-done:
				return
			case intStream <- n:
			}
		}
	}()

	return heartbeat, intStream
}
```

With `time.Sleep` we simulate delays before the goroutine starts working. It can be all kind of things..CPU load, disk contention, network latency, etc.

## Replicated Requests ##

This is useful when the applications top priority is receiving a response as fast as possible. Maybe it's servicing a user's HTTP request, or retrieving a blob of data.

In these instances, you can replicate the request to multiple handlers (i.e. goroutines, processes, servers) and one of them will return faster than the others. Of course, the downside is that you are utilizing way more resources for this whole process.

For example, replicating a simulated request over 10 handlers:

```
doWork := func(
    done <-chan interface{},
    id int,
    wg *sync.WaitGroup,
    result chan<- int,
) {
    started := time.Now()
    defer wg.Done()

    // Simulate random load
    simulatedLoadTime := time.Duration(1+rand.Intn(5))*time.Second
    select {
    case <-done:
    case <-time.After(simulatedLoadTime):
    }

    select {
    case <-done:
    case result <- id:
    }

    took := time.Since(started)
    // Display how long handlers would have taken
    if took < simulatedLoadTime {
    took = simulatedLoadTime
    }
    fmt.Printf("%v took %v\n", id, took)
}

done := make(chan interface{})
result := make(chan int)

var wg sync.WaitGroup
wg.Add(10)

for i:=0; i < 10; i++ {
    go doWork(done, i, &wg, result)
}

firstReturned := <-result
close(done)
wg.Wait()
fmt.Printf("Received an answer from #%v\n", firstReturned)
```

Bare in mind that all the handlers need to have equal opportunity to service the request. Also, keep in mind the uniformity of your handlers. You should only replicate the requests to handlers that have different runtime conditions: different processes, machines, paths to a data store, access, etc.

## Rate limiting ##

Rate limiting deals with constraining the number of times some kind of resource is accesed to some finite number per unit of time. This can refer to API connections, disk reads/writes, network packets, errors, etc.

Main motives are malicious use, user degrading the overall performance of a distributed system. This applies even to *one* user since a lot of time systems have been developed to work well under the common use case..as soon as we have weird, different circumstances the effect can cascade through the whole system.

---

Most rate limiting is done by utilizing an algorithm called the *token bucket*. A useful mental model for this is to imagine using an *access token* for a resource we want to get. These tokens are stored in a bucket waiting to be retrieved for usage. The bucket has a depth of `d` which indicates it can hold `d` access tokens at a time. 

Therefore, five tokens gives you the possibility to access the resource five times. We define `r` to be the rate at which the tokens are added *back* to the bucket. By handling `d` and `r` we control the *burstiness* (how many requests can be made when the bucket is full) and overall rate limit.

Simple rate limiter:

```
	defer log.Printf("Done.")
	log.SetOutput(os.Stdout)
	log.SetFlags(log.Ltime | log.LUTC)

	apiConnection := Open()
	var wg sync.WaitGroup
	wg.Add(20)

	for i := 0; i < 10; i++ {
		go func() {
			defer wg.Done()
			err := apiConnection.ReadFile(context.Background())
			if err != nil {
				log.Printf("cannot ReadFile: %v", err)
			}
			log.Printf("ReadFile")
		}()
	}

	for i := 0; i < 10; i++ {
		go func() {
			defer wg.Done()
			err := apiConnection.ResolveAddress(context.Background())
			if err != nil {
				log.Printf("cannot ResolveAddress: %v", err)
			}
			log.Printf("ResolveAddress")
		}()
	}

	wg.Wait()
}
func Open() *APIConnection {
	return &APIConnection{
		rateLimiter: rate.NewLimiter(rate.Limit(1), 1), 
	}
}

type APIConnection struct {
	rateLimiter *rate.Limiter
}

func (a *APIConnection) ReadFile(ctx context.Context) error {
	if err := a.rateLimiter.Wait(ctx); err != nil { 
		return err
	}
	// Pretend we do work here
	return nil
}

func (a *APIConnection) ResolveAddress(ctx context.Context) error {
	if err := a.rateLimiter.Wait(ctx); err != nil { 
		return err
	}
	// Pretend we do work here
	return nil
}
```